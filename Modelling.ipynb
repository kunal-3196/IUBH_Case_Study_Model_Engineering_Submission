{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling : Credit Card Routing for Online Purchase via Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating entropy for dataset\n",
    "The dataset given here is based on success & failure of a given transaction, so ideally it is the **probability** of success which can help us to start the model. Furthere, mathematical function called **entropy** which identifies degree of randomness in dataset can be applied here to select features & split the entire file.\n",
    "\n",
    "**log2**= library imported for calculating log with base 2\n",
    "\n",
    "**entropy**=-$\\sum_{i}^n P_i(log)_2(P_i) $\n",
    "\n",
    "**n**=total number of type transactions (2 in our case i.e. success & failure), **i**=individual type of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing library for calculating log with base 2\n",
    "from math import log2\n",
    "#defining entropy function\n",
    "def class_entropy(k,n):\n",
    "    #Probability of success\n",
    "    P_Success = k/n\n",
    "    #Probability of failure\n",
    "    P_Fail = (n-k)/n\n",
    "    #Applying enropy formula\n",
    "    entropy=-((P_Success*log2(P_Success))+(P_Fail*log2(P_Fail)))\n",
    "    return entropy\n",
    "#Calculating total number of data in the set\n",
    "N=len(pd_merge_t_fee_sorted)\n",
    "#Calculating total number of transaction success records in the dataset\n",
    "K=len(pd_merge_t_fee_sorted[pd_merge_t_fee_sorted[\"success\"]==1])\n",
    "#Calling entropy function\n",
    "Total_entropy_value=class_entropy(K, N)\n",
    "print(\"The total class entropy is\",format(feature_info_gain,\"0.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Information gain based on entropy\n",
    "It is an assumption that **PSP** & **Country** are the two deciding features used for spliting the data & the priority of these features can be determined through **information gain**\n",
    "The entropy of for each feature will be calculated separately by calling the entrpy function for successful record. However, the weighted sum of the entropy for each feature will generate total entropy. It is determined using below code. The weighted sum is also called **split info** which is stored in **Total_feature_entropy_value** for each feature.  \n",
    "\n",
    "**Total_feature_entropy_value**=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating information gain for the features passed in feature list.\n",
    "def info_gain(features_list):\n",
    "    #Iterating through all the features in the list\n",
    "    for i in range(len(features_list)):\n",
    "        #Collecting unique values for each feature in a separate list\n",
    "        feature_list=list(set(pd_merge_t_fee_sorted[features_list[i]]))\n",
    "        #Initializing individual lists for probability with parent dataset, split info & vatiable for information gain value\n",
    "        feature_probability=[]\n",
    "        feature_entropy_list=[]\n",
    "        Total_feature_entropy_value=0\n",
    "        #Iterating through all the features in the feature list\n",
    "        for j in range(len(feature_list)):\n",
    "            #Collecting sample size of total & successful data for each feature\n",
    "            n=len(pd_merge_t_fee_sorted[pd_merge_t_fee_sorted[features_list[i]]==feature_list[j]])\n",
    "            k=len(pd_merge_t_fee_sorted[(pd_merge_t_fee_sorted[features_list[i]]==feature_list[j]) & (pd_merge_t_fee_sorted[\"success\"]==1)])\n",
    "            #Calculating feature probability based on total sample size and appending to the list\n",
    "            feature_probability.append(k/N)\n",
    "            #Calling class entropy based on child dataset specific to sample size of that feature and appending to the list\n",
    "            feature_entropy_list.append(class_entropy(k,n))\n",
    "        #Iterating through all the information gain values calculated above\n",
    "        for j in range(len(feature_entropy_list)):\n",
    "            #Calculating split info based on parent sample size for each feature by multiplying with the total probability\n",
    "            Total_feature_entropy_value+=feature_entropy_list[j]*feature_probability[j]\n",
    "        #Calculating information gain for each feature\n",
    "        feature_info_gain=Total_entropy_value-Total_feature_entropy_value\n",
    "        print(\"Information Gain for\",features_list[i],\"is\",format(feature_info_gain,\"0.3f\"))\n",
    "#Selecting below features for performing the analysis\n",
    "features_list=[\"PSP\",\"country\"]\n",
    "#Calling the function\n",
    "info_gain(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determination of transaction fee using Predictive Model\n",
    "\n",
    "The predictive model is goverend by different steps as below:\n",
    "\n",
    "**Data transformation**: Here, given data will be converted into a dataframe where number of attempts for each duplicate transaction will be recorded.\n",
    "\n",
    "**Objective function**: Objective function will be determined based on the conditions where business can generate better revenue thereby lowering the transaction fees for each PSP.\n",
    "\n",
    "**Model Creation**: This will work on a predictive analysis where predicted PSP & predicted transaction amount will be calculated by using multiple dataframes thorough series of operations.\n",
    "\n",
    "**dataframe.insert(a,b,c)** = inserts column *b* at *a* index with *c* as its initial values\n",
    "\n",
    "**np.arange(a,b,c)**= genrates list from number *a* to *b* with a common difference of *c* between the values\n",
    "\n",
    "**dataframe.rename(columns={a:b},inplace=True)**=renames column header from *a* to *b*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log2\n",
    "import numpy as np\n",
    "\n",
    "#Function which returns dataframe of transaction fee merged with each transaction\n",
    "def merge_transac_fee(df_original,df_transac_fee,PSP,transaction_fees_col):\n",
    "    #Filtering failed records from all the given original transaction dataset\n",
    "    df_original_fail=df_original[df_original[\"success\"]==0]\n",
    "    #dropping column of success transaction fees from the provided reference\n",
    "    df_transac_fee_fail=df_transac_fee.drop(columns=(\"Success\"))\n",
    "    #Merging the filtered failed transaction dataset with the reference of transaction fees\n",
    "    pd_merge_fail=pd.merge(df_original_fail, df_transac_fee_fail, on=PSP,how=\"inner\")\n",
    "    #Remaning the column to the value passed in the function\n",
    "    pd_merge_fail.rename(columns={\"Fail\":transaction_fees_col},inplace=True)\n",
    "    #Repeating above steps to merge the transaction fees in a dataframe for successful records\n",
    "    df_original_success=df_original[df_original[\"success\"]==1]\n",
    "    df_transac_fee_success=df_transac_fee.drop(columns=(\"Fail\"))\n",
    "    pd_merge_success=pd.merge(df_original_success, df_transac_fee_success, on=PSP,how=\"inner\")\n",
    "    pd_merge_success.rename(columns={\"Success\":transaction_fees_col},inplace=True)\n",
    "    #Concatinating child datasets for successful & failed transactions to generate parent dataframe\n",
    "    pd_merge_t_fee=pd.concat([pd_merge_fail,pd_merge_success])\n",
    "    #Sorting the parent dataframe based on timestamp & returning\n",
    "    pd_merge_t_fee_sorted=pd_merge_t_fee.sort_values(by=[\"time_diff_cum\"])\n",
    "    return(pd_merge_t_fee_sorted)\n",
    "\n",
    "#Funtion to obtain try attempts to complete same transaction\n",
    "def try_count(merge_transac_fee_dframe):\n",
    "    #Dropping unnecessary columns\n",
    "    merge_transac_fee_dframe=merge_transac_fee_dframe.drop(columns=([\"Unnamed: 0\",\"tmsp\",\"3D_secured\",\"card\"]))\n",
    "    #Inserting new column called as try count\n",
    "    merge_transac_fee_dframe.insert(6, \"try_count\",1)\n",
    "    #Iterating through entire dataframe\n",
    "    for i in range(1,len(merge_transac_fee_dframe)):\n",
    "           #Checking for consecutive transactions with same amount given in first column\n",
    "           if(merge_transac_fee_dframe.iat[i, 1]==merge_transac_fee_dframe.iat[i-1, 1]):\n",
    "               #Incrementing count of attempts under try count column\n",
    "               merge_transac_fee_dframe.iat[i,6]=merge_transac_fee_dframe.iat[i-1,6]+1\n",
    "    #Returning final dataframe with number of tries\n",
    "    return(merge_transac_fee_dframe)\n",
    "\n",
    "#Function for predicting PSP with minimum transaction fee\n",
    "def predicted_PSP(data_try_count):\n",
    "    #Inserting new column for determining predicted PSP\n",
    "    data_try_count.insert(7,\"predicted_PSP\",\"predicted_PSP\")\n",
    "    #For every failed transaction at first attempt\n",
    "    Simplecard=np.arange(1,1000,4)\n",
    "    #For every failed transaction at second attempt\n",
    "    UK_card=np.arange(2,1000,4)\n",
    "    #For every failed transaction at third attempt\n",
    "    Moneycard=np.arange(3,1000,4)\n",
    "    #For every failed transaction at fourth attempt\n",
    "    Goldcard=np.arange(4,1000,4)\n",
    "    #Iterating through the dataframe having number of try counts\n",
    "    for i in range(len(data_try_count)):\n",
    "        #Identifying & updating predicted_PSP value based on matched condition\n",
    "        if(data_try_count.iat[i,6] in(Simplecard)):\n",
    "            data_try_count.iat[i,7]=\"Simplecard\"\n",
    "        if(data_try_count.iat[i,6] in(UK_card)):\n",
    "            data_try_count.iat[i,7]=\"UK_Card\"\n",
    "        if(data_try_count.iat[i,6] in(Moneycard)):\n",
    "            data_try_count.iat[i,7]=\"Moneycard\"\n",
    "        if(data_try_count.iat[i,6] in(Goldcard)):\n",
    "            data_try_count.iat[i,7]=\"Goldcard\"\n",
    "    #Returning updated dataframe with predicted PSP\n",
    "    predicted_card_count=data_try_count\n",
    "    return(predicted_card_count)\n",
    "\n",
    "#Function for calculating predicted transaction fees\n",
    "def predicted_transaction_amount(predicted_PSP,transac_fee):\n",
    "    #Calling merge function to obtain transaction fees from predicted PSP\n",
    "    return(merge_transac_fee(predicted_PSP,transac_fee,\"predicted_PSP\",\"predicted_transaction_fees\"))\n",
    "#Specifying path of Original dataset & transaction fee for PSP given by IU from local system to a dataframe\n",
    "df_original=pd.DataFrame(pd.read_csv(\"C:\\\\MISC\\\\IU_downloads\\\\PSP_Jan_Feb_2019_original.csv\"),index=None)\n",
    "df_transac_fee=pd.DataFrame(pd.read_csv(\"C:\\\\MISC\\\\IU_downloads\\\\PSP_Jan_Feb_2019_transac_fees.csv\"),index=None)\n",
    "#Calling predicted PSP function by passing abouve dataframes in the arguement\n",
    "predicted_PSP_data=predicted_PSP(try_count(merge_transac_fee(df_original,df_transac_fee,\"PSP\",\"original_transaction_fees\")))\n",
    "#Renaming PSP column to predicted PSP & passing to a function for predicting new transaction amount\n",
    "df_transac_fee.rename(columns={\"PSP\":\"predicted_PSP\"},inplace=True)\n",
    "pred_t_fee_dframe=predicted_transaction_amount(predicted_PSP_data,df_transac_fee)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
